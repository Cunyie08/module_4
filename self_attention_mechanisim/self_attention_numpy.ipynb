{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1a083e",
   "metadata": {},
   "source": [
    "### Using Numpy to Create self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7fe3e",
   "metadata": {},
   "source": [
    " Think of Self-Attention Like a Classroom Discussion\n",
    "Imagine you are in a classroom discussing the sentence:\n",
    "\n",
    " **\"The cat sat on the mat.\"**\n",
    "\n",
    "You are the teacher, and you want to understand what each word in the sentence means in context by looking at all the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5466fe0",
   "metadata": {},
   "source": [
    " 1: Understanding the Need for Attention\n",
    "Each word in a sentence has a meaning, but its meaning depends on other words too.\n",
    "\n",
    "Example:\n",
    "\n",
    "The word \"sat\" means \"to be in a sitting position.\"\n",
    "But who is sitting? (\"cat\" is the subject).\n",
    "Where? (\"on the mat\" tells us the location)\n",
    "\n",
    "Instead of looking at words one by one, self-attention allows each word to \"ask\" other words for relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0caa2",
   "metadata": {},
   "source": [
    "Step 2: Assigning Roles – Queries, Keys, and Values\n",
    "To help each word decide what to focus on, we give each word three roles:\n",
    "\n",
    "1. Query (Q) – The Question:\n",
    "\n",
    "Each word \"asks\" about the meaning of the sentence from its perspective.\n",
    "Example: \"What is important for me to understand?\"\n",
    "\n",
    "2. Key (K) – The Information Holder:\n",
    "\n",
    "Each word \"offers\" its meaning to others.\n",
    "Example: \"This is what I mean, take it if needed.\"\n",
    "\n",
    "3. Value (V) – The Final Meaning:\n",
    "\n",
    "Each word carries useful information that will be passed to others.\n",
    "\n",
    "Think of this like students asking and answering questions in a classroom.\n",
    "\n",
    "If you are the student (Query), you ask important questions (Q).\n",
    "The other students (Keys) provide answers (K).\n",
    "You collect and process those answers (V) to understand better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425fc36",
   "metadata": {},
   "source": [
    "Step 3: Scoring the Importance of Words\n",
    "Now, each word \"talks\" to every other word and decides how important they are.\n",
    "\n",
    "How? By comparing Queries (Q) and Keys (K)!\n",
    "\n",
    "If a word’s Query (Q) matches well with another word’s Key (K), that means it’s important.\n",
    "The higher the match, the more attention it gets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac02a2",
   "metadata": {},
   "source": [
    " Step 4: Making the Attention Weights\n",
    "Now that we have scores, we want to convert them into a proper weight (like a probability).\n",
    "\n",
    "How? By applying softmax():\n",
    "\n",
    "It makes the most important words stand out (high scores become bigger, low scores become smaller).\n",
    "The total attention across all words sums to 1 (100%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf03c0",
   "metadata": {},
   "source": [
    "Step 5: Updating the Meaning Using Values (V)\n",
    "Each word now mixes information from other words using the attention weights.\n",
    "\n",
    "Each word’s final meaning is a blend of the words it pays attention to!\n",
    "\n",
    "Example:\n",
    "\n",
    "\"sat\" borrows information from \"cat\" and \"on\" (because they got high attention scores).\n",
    "This helps \"sat\" understand that it refers to the cat sitting on something."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4072f30",
   "metadata": {},
   "source": [
    "```\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "```\n",
    "`Attention_Scores = Q @ K.T\n",
    "`\n",
    "\n",
    "`Scaled_Scores = Attention_Scores / sqrt(d_k)\n",
    "`\n",
    "\n",
    "`Attention_Weights = softmax(Scaled_Scores)\n",
    "`\n",
    "\n",
    "`Output = Attention_Weights @ V\n",
    "`\n",
    "\n",
    "```\n",
    "Self_Attention(Q, K, V) = softmax((Q @ K.T) / sqrt(d_k)) @ V\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ff6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05de8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Word Embeddings: [[1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Define the input (Word embeddings)\n",
    "x = np.array([[1, 0, 1], # word 1\n",
    "              [0, 1, 1], # word 2\n",
    "              [1, 1, 0] # word 3\n",
    "])\n",
    "\n",
    "print(\"Input Word Embeddings:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b7ef823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Query(Q), Keys(K), Values(V) matrices using random numbers\n",
    "q = np.random.rand(3,3)\n",
    "k = np.random.rand(3,3)\n",
    "v = np.random.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c18c95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18347741 0.41284234 0.78488908]\n",
      " [0.49234389 0.69207146 0.17371277]\n",
      " [0.68670259 0.70446771 0.06658257]]\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ceccc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18347741 0.41284234 0.78488908]\n",
      "[0.49234389 0.69207146 0.17371277]\n",
      "[0.68670259 0.70446771 0.06658257]\n"
     ]
    }
   ],
   "source": [
    "print(q[0]) # displays the first row\n",
    "print(q[1]) # displays the second row\n",
    "print(q[2]) # displays the third row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7011ea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 3\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the row\n",
    "print(\"row:\", q.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fe1405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 3\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the column\n",
    "print(\"Column:\", q.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db3336af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the q,k,v using dot product or (@)\n",
    "q_x = np.dot(x, q)\n",
    "k_x = np.dot(x, k)\n",
    "v_x = np.dot(x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0998fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      " [[0.87018    1.11731005 0.85147165]\n",
      " [1.17904648 1.39653917 0.24029534]\n",
      " [0.67582129 1.10491381 0.95860185]] \n",
      "\n",
      "Key:\n",
      " [[0.91529893 1.54382904 1.54427484]\n",
      " [1.25577347 1.45645986 0.77585783]\n",
      " [0.55500505 1.77899187 1.08786789]] \n",
      "\n",
      "Value:\n",
      " [[1.96157533 0.52258213 1.37471803]\n",
      " [1.3899051  0.72213917 0.85464171]\n",
      " [1.36134791 0.27962817 0.90030526]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\\n\",q_x, \"\\n\")\n",
    "print(\"Key:\\n\",k_x, \"\\n\")\n",
    "print(\"Value:\\n\",v_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf7ea88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the attention scores using dot product and Transpose\n",
    "attention_scores = np.dot(q_x, k_x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "699f090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.83631675 3.38068715 3.39692845]\n",
      " [3.60627974 3.70105356 3.40021816]\n",
      " [3.80472123 3.20167982 3.38354908]]\n"
     ]
    }
   ],
   "source": [
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2ee0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "935dc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the softmax to get the attention weights\n",
    "attention_weights = softmax(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d2e3e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43888925, 0.27827713, 0.28283362],\n",
       "       [0.34326595, 0.37739007, 0.27934398],\n",
       "       [0.4538395 , 0.24831602, 0.29784448]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa52d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ouput after Self-attention:\n",
      "\n",
      " [[1.63272808 0.50939874 1.0958128 ]\n",
      " [1.57816274 0.53002525 1.04592204]\n",
      " [1.64084603 0.49977284 1.10427352]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the final output (weighted sum of values)\n",
    "output = attention_weights @ v_x\n",
    "print(\"Final Ouput after Self-attention:\\n\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
