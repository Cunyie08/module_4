{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324ebb82",
   "metadata": {},
   "source": [
    "#### Prompting Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a2c626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the key concepts of the text delimited by triple backticks in simple terms ```Machine learning is a field of artificial intelligence that enables computers to learn from data and make predictions```\n"
     ]
    }
   ],
   "source": [
    "text = \"Machine learning is a field of artificial intelligence that enables computers to learn from data and make predictions\"\n",
    "prompt = f\"\"\"Explain the key concepts of the text delimited by triple backticks in simple terms ```{text}```\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b171f0",
   "metadata": {},
   "source": [
    "#### Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46602dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import base64\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea2b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Il fait beau aujourd'hui.**\n",
      "\n",
      "*(A slightly more literal, but less common, translation would be: Le temps est agrÃ©able aujourd'hui.)*\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"Translate the following English sentence into French: 'The weather is nice today'\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07c74a",
   "metadata": {},
   "source": [
    "#### One-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edca8801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language: \"Spanish\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"\"\"Detect the language of the following sentences:\n",
    "\"text: A plus tard\"  -> language: \"French\n",
    "Now detect:\n",
    "text: \"Te amo!\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01526c",
   "metadata": {},
   "source": [
    "#### Few-Shots prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb587839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Negative\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt =\"\"\"Determine the sentiment of the following sentences:\n",
    "text: \"I love this product! It works perfectly.\" -> classification: Positive\n",
    "text: \"The service was terrible. I'm very disappointed.\" -> Classification: Negative\n",
    "text: \"The food was okay, nothing special.\" -> Classification: Neutral\n",
    "Now analyze this sentence:\n",
    "\"Data annotation is so stressful for me, i have to do it anyway!\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc39c71",
   "metadata": {},
   "source": [
    "#### Multi-Step Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf4979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Input Text:** President Muhammed Buhari governed Nigeria for 8 years\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Identify the key entities in the sentence.\n",
      "\n",
      "1.  President Muhammed Buhari\n",
      "2.  Nigeria\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Classify each entity as a person, event, or object.\n",
      "\n",
      "| Entity | Classification |\n",
      "| :--- | :--- |\n",
      "| President Muhammed Buhari | Person |\n",
      "| Nigeria | Object (Geopolitical Entity/Location) |\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "text = \"President Muhammed Buhari governed Nigeria for 8 years\"\n",
    "prompt = f\"\"\"You will be provided with a text delimited by triple backticks.\n",
    "Step 1: Identify the key entities in the sentence.\n",
    "Step 2:\n",
    "Classify each entity as a person, event, or object.\n",
    "{text}\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83fe1d9",
   "metadata": {},
   "source": [
    "#### Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be9ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the step-by-step solution:\n",
      "\n",
      "1. **Starting amount:** John begins with 3 apples.\n",
      "2. **Apples bought:** He buys 5 more apples.\n",
      "   * Calculation: $3 + 5 = 8$ apples.\n",
      "3. **Apples given away:** He gives away 2 apples.\n",
      "   * Calculation: $8 - 2 = 6$ apples.\n",
      "\n",
      "**Answer:** John has **6** apples now.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt= \"John has 3 apples. He buys 5 more and gives away 2. How many apples does he have now? Think step by step.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8c5b4",
   "metadata": {},
   "source": [
    "#### Self-Consistency prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "instruction = \"Imagine three completely independent experts who reason differently are answering this question. The final answer is obtained by majority vote. The question is: \"\n",
    "question = \"A bookstore has 20 books on a shelf. A customer buys 5 books. Then, the store restocks with double the number of books bought. After that, another customer buys 8 books. How many books are left on the shelf?\"\n",
    "prompt = instruction + question\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea612a41",
   "metadata": {},
   "source": [
    "#### Role playing prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bb1de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great. Preparing effectively is key. I'll act as the hiring manager for a Senior Data Scientist role. I'm focused on assessing your technical depth, business acumen, and ability to communicate complex ideas clearly.\n",
      "\n",
      "Let's begin with the first question.\n",
      "\n",
      "---\n",
      "\n",
      "### Question 1: Project Execution and Model Choice\n",
      "\n",
      "Walk me through a recent project where you took a raw business problem, translated it into a machine learning task, and successfully deployed a solution. Focus specifically on how you selected the appropriate model (e.g., Logistic Regression vs. XGBoost vs. Neural Network) and how you justified that choice to non-technical stakeholders.That sounds like incredibly vital, demanding, and complex work. Thank you for providing that context.\n",
      "\n",
      "As an experienced hiring manager, I am going to focus on assessing your ability to handle stress, manage complexity, and drive results. We will proceed one question at a time, followed by detailed feedback.\n",
      "\n",
      "Here is the first question:\n",
      "\n",
      "### Question 1\n",
      "\n",
      "**That experience suggests you dealt with high levels of uncertainty. Can you describe a time when operational parameters (like staffing, safety protocols, or supply chain requirements) shifted suddenly and dramatically due to the pandemic? How did you prioritize the immediate changes, communicate the new procedures to your team or peers, and measure the success of the transition?**That's an excellent role to play. As an experienced hiring manager, my goal is to understand not just your skills, but your judgment, motivation, and fit within our culture.\n",
      "\n",
      "I will ask you five questions sequentially. Please answer each one fully before I provide feedback and move on to the next.\n",
      "\n",
      "Let's begin.\n",
      "\n",
      "***\n",
      "\n",
      "### Question 1:\n",
      "\n",
      "**Tell me about a time when you had to take ownership of a significant project or responsibility that was outside your core job description. What motivated you to take on the challenge, and what was the ultimate result of your involvement?**Welcome! I'm glad we could connect today. I'll be guiding you through a series of questions focused on your experience, problem-solving abilities, and how you approach challenges.\n",
      "\n",
      "I will ask five questions, one at a time, and provide specific feedback after each of your answers.\n",
      "\n",
      "Let's begin with the first question.\n",
      "\n",
      "***\n",
      "\n",
      "### Question 1\n",
      "\n",
      "**\"Walk me through your background and experience, focusing specifically on how it has prepared you for the responsibilities outlined in this role. What are you hoping to achieve by making this move?\"**"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     36\u001b[39m     user_prompt = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUser:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(user_prompt)\u001b[39m\n\u001b[32m     13\u001b[39m contents = [\n\u001b[32m     14\u001b[39m     types.Content(\n\u001b[32m     15\u001b[39m         role=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     ),\n\u001b[32m     20\u001b[39m ]\n\u001b[32m     21\u001b[39m generate_content_config = types.GenerateContentConfig(\n\u001b[32m     22\u001b[39m     system_instruction=[\n\u001b[32m     23\u001b[39m         types.Part.from_text(text=system_prompt),\n\u001b[32m     24\u001b[39m     ],\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerate_content_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\google\\genai\\models.py:5395\u001b[39m, in \u001b[36mModels.generate_content_stream\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5389\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m1\u001b[39m:\n\u001b[32m   5392\u001b[39m   \u001b[38;5;66;03m# First request gets a function call.\u001b[39;00m\n\u001b[32m   5393\u001b[39m   \u001b[38;5;66;03m# Then get function response parts.\u001b[39;00m\n\u001b[32m   5394\u001b[39m   \u001b[38;5;66;03m# Yield chunks only if there's no function response parts.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5395\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5396\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunction_map\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5397\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_extra_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend_chunk_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\google\\genai\\models.py:4089\u001b[39m, in \u001b[36mModels._generate_content_stream\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4081\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4082\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4083\u001b[39m ):\n\u001b[32m   4084\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4085\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mAccessing the raw HTTP response is not supported in streaming\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   4086\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m methods.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   4087\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m4089\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_streamed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4090\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4091\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   4093\u001b[39m \u001b[43m  \u001b[49m\u001b[43mresponse_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4095\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1405\u001b[39m, in \u001b[36mBaseApiClient.request_streamed\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest_streamed\u001b[39m(\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1396\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1399\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1400\u001b[39m ) -> Generator[SdkHttpResponse, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m   1401\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1402\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1403\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m   session_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m session_response.segments():\n\u001b[32m   1407\u001b[39m     chunk_dump = json.dumps(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1188\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m   1181\u001b[39m   httpx_request = \u001b[38;5;28mself\u001b[39m._httpx_client.build_request(\n\u001b[32m   1182\u001b[39m       method=http_request.method,\n\u001b[32m   1183\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1187\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttpx_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m   errors.APIError.raise_for_response(response)\n\u001b[32m   1190\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1191\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1192\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kanyi\\OneDrive\\Desktop\\module_4\\prompt_engineering\\a_venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "system_prompt = \"Act as an experienced hiring manager. Ask me five interview questions one by one. After I respond, provide constructive feedback on my answer, including strengths and areas for improvement. If my response is incomplete, guide me toward a better answer.\"\n",
    "user_prompt = \"I am preparing for a Data Scientist interview. Can you ask me fine five questions and evaluate my responses?\"\n",
    "\n",
    "def generate(user_prompt):\n",
    "    client = genai.Client(\n",
    "        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "\n",
    "    model = \"gemini-flash-latest\"\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_text(text=user_prompt),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        system_instruction=[\n",
    "            types.Part.from_text(text=system_prompt),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for chunk in client.models.generate_content_stream(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    ):\n",
    "        print(chunk.text, end=\"\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_prompt = input(\"\\nUser:\")\n",
    "    generate(user_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e575b",
   "metadata": {},
   "source": [
    "#### F-string prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59af9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a birthday message for Timi, who is turning 14.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae7f8a",
   "metadata": {},
   "source": [
    "name = \"Timi\"\n",
    "age = 14\n",
    "prompt = f\"write a birthday message for (name), who is turning (age).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235b49cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy 14th Birthday, Timi! ðð\n",
      "\n",
      "Hope you have an amazing day celebrating! This is a big yearâenjoy every minute of being 14. Wishing you tons of fun, laughter, and all the best as you head into the next chapter!\n",
      "\n",
      "Have a fantastic birthday! ð¥³\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "name = \"Timi\"\n",
    "age = 14\n",
    "prompt = f\"Write a birthday message for {name}, who is turning {age}.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ada8e",
   "metadata": {},
   "source": [
    "#### Using JSON with prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87c1305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John', 'age': 16, 'interest': 'coding'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "\"name\": \"John\",\n",
    "\"age\": 16,\n",
    "\"interest\": \"coding\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e848a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt: \n",
      " My name is Zara. I love coding and want to\n",
      "learn AI. Write a friendly and short guide on how I can\n",
      "start.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "user_profile = {\n",
    "\n",
    "\"name\": \"Zara\",\n",
    "\"interest\": \"coding\",\n",
    "\"goal\": \"learn AI\",\n",
    "\"tone\": \"friendly\",\n",
    "\"length\": \"short\"\n",
    "\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"My name is {user_profile['name']}. I love {user_profile['interest']} and want to\n",
    "{user_profile['goal']}. Write a {user_profile['tone']} and {user_profile['length']} guide on how I can\n",
    "start.\"\"\"\n",
    "print(\"Generated Prompt: \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef903f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "user_profile = {\n",
    "\"name\": \"Zara\",\n",
    "\"Interest\": \"coding\",\n",
    "\"goal\": \"learn AI\",\n",
    "\"tone\": \"Friendly\",\n",
    "\"length\": \"short\"}\n",
    "prompt = f\"\"\"My name is {user_profile['name']}. I love {user_profile ['interest']} and want to\n",
    "{user_profile['goal']}. write a {user_profile['tone']} and {user_profile['length']} guide on how 1 can start.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f41c85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Zara! That's fantastic! Loving coding is the perfect launchpad for diving into the world of AI.\n",
      "\n",
      "AI can seem huge, but we can break it down into four simple, actionable steps. Here is your friendly, short guide to getting started:\n",
      "\n",
      "---\n",
      "\n",
      "## ð Your 4-Step AI Launch Guide\n",
      "\n",
      "### Step 1: Master the AI Language (Python)\n",
      "\n",
      "Since you already code, this will be easy! Python is the undisputed king of AI.\n",
      "\n",
      "*   **Focus:** Ensure you are comfortable with Python fundamentals.\n",
      "*   **Key Libraries:** Immediately familiarize yourself with **NumPy** (for numerical operations) and **Pandas** (for data manipulation). These are the tools you'll use every single day.\n",
      "\n",
      "### Step 2: Understand the Data (The Fundamentals)\n",
      "\n",
      "AI is all about data and statistics. You don't need a PhD in math, but you need the basics.\n",
      "\n",
      "*   **Focus:** Learn the fundamentals of **Data Science**.\n",
      "*   **Key Concepts:** Understand basic statistics (mean, median, standard deviation) and the concept of **Linear Regression** (the simplest form of machine learning).\n",
      "\n",
      "### Step 3: Build Your First Model (Hands-On Practice)\n",
      "\n",
      "Time to get your hands dirty! You need a beginner-friendly toolkit.\n",
      "\n",
      "*   **The Toolkit:** Start using the **Scikit-learn** library. It's the easiest way to implement classic Machine Learning algorithms without writing complex code from scratch.\n",
      "*   **Your First Project:** Try the famous **Iris Dataset** or the **Titanic Survival Prediction** challenge on Kaggle. These are perfect \"Hello World\" projects for ML.\n",
      "\n",
      "### Step 4: Choose Your Path (Machine Learning vs. Deep Learning)\n",
      "\n",
      "Once you have the basics down, you can decide where you want to specialize.\n",
      "\n",
      "*   **Machine Learning (ML):** Focuses on traditional algorithms (good for structured data).\n",
      "*   **Deep Learning (DL):** Focuses on Neural Networks (essential for images, text, and complex tasks).\n",
      "*   **Next Tool:** If you choose DL, start exploring **TensorFlow** or **PyTorch**.\n",
      "\n",
      "---\n",
      "\n",
      "**Remember, Zara:** AI is learned best by *doing*. Don't worry about understanding every single mathematical detail right away. Start coding, start building, and the theory will follow! Happy learning!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "user_profile = {\n",
    "\"name\": \"Zara\",\n",
    "\"Interest\": \"coding\",\n",
    "\"goal\": \"learn AI\",\n",
    "\"tone\": \"Friendly\",\n",
    "\"length\": \"short\"\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"My name is {user_profile['name']}. I love {user_profile ['Interest']} and want to\n",
    "{user_profile['goal']}. write a {user_profile['tone']} and {user_profile['length']} guide on how 1 can start.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633988fb",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf838bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a classic GTA Online interaction, moving seamlessly from in-game aggression to social media toxicity.\n",
      "\n",
      "---\n",
      "\n",
      "## GTA Online Tweet Fight Analysis\n",
      "\n",
      "**The Exchange:**\n",
      "*@LS_King: Yo why'd you blow up my car?!*\n",
      "*@ThugLife99: Cry about it #GetRekt*\n",
      "\n",
      "### 1. Who started it?\n",
      "\n",
      "**@ThugLife99** started the conflict.\n",
      "\n",
      "While @LS_King initiated the *tweet thread* by asking a question, @ThugLife99 was the aggressor who started the *entire conflict* by committing the in-game act of blowing up the car. Furthermore, @ThugLife99 immediately escalated the verbal fight with a dismissive and antagonistic response, confirming their role as the primary instigator of the toxicity.\n",
      "\n",
      "### 2. Most toxic word?\n",
      "\n",
      "**\"Cry about it.\"**\n",
      "\n",
      "While `#GetRekt` is aggressive and mocking, \"Cry about it\" is far more toxic. It is a direct, condescending dismissal of the opponent's legitimate frustration, attacking their emotional state and implying that their anger is childish and unwarranted. It is the ultimate form of griefing validation.\n",
      "\n",
      "### 3. Suggested Peace Treaty (Funny)\n",
      "\n",
      "**The \"Passive Mode Primo\" Treaty**\n",
      "\n",
      "1.  **Reparations:** @ThugLife99 must purchase @LS_King a replacement vehicle, specifically a fully customized, gold-plated **Albany Primo Custom**.\n",
      "2.  **Mandatory Bonding:** Both players must enter Passive Mode and drive the Primo together (one driving, one passenger) from the Los Santos Customs in Burton to the very top of Mount Chiliad.\n",
      "3.  **The Soundtrack:** During the entire drive, the radio must be locked exclusively to **Blaine County Radio**.\n",
      "4.  **The Penalty:** If either player attempts to use an Oppressor Mk II, a jet, or a sticky bomb during the drive, the treaty is void, and they must immediately meet at the Maze Bank Tower helipad for a mandatory, non-consensual in-game hug.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "text = \"\"\"@LS_King: Yo why'd you blow up my car?! @ThugLife99: Cry about it #GetRekt\"\"\"\n",
    "prompt = f\"\"\"Analyze this GTA Online\n",
    "tweet fight:\n",
    "1. Who started it?\n",
    "2. Most toxic word?\n",
    "3. Suggested peace treaty (funny): ```{text}```\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd023f34",
   "metadata": {},
   "source": [
    "#### STructured Output prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e83caf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Company | Industry | Stock Performance |\n",
      "|---|---|---|\n",
      "| Tesla | Electric Vehicles | Increased by 20% in the last year |\n",
      "| Apple | Technology | Increased by 15% in the last year |\n",
      "| Amazon | E-commerce and Cloud Computing | Increased by 10% in the last year |\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "text= \"Tesla is an electric vehicle company. Its stock price has increased by 20% in the last year. Apple is a technology company known for its iphones. Its stock price has increased by 15% in the last year. Amazon is an e-commerce and cloud computing giant. Its stock price has increased by 10% in the last year.\"\n",
    "prompt= f\"\"\"Convert the following information into a table format with columns for company, industry and stock performance:\n",
    "Text: ```{text}```\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59602eed",
   "metadata": {},
   "source": [
    "| Company | Industry | Stock Performance |\n",
    "|---|---|---|\n",
    "| Tesla | Electric Vehicle | Increased by 20% in the last year |\n",
    "| Apple | Technology | Increased by 15% in the last year |\n",
    "| Amazon | E-commerce and Cloud Computing | Increased by 10% in the last year |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f344e2",
   "metadata": {},
   "source": [
    "#### List (Structured output Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fde9fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   **Healthcare:** AI is transforming this industry by improving efficiency and decision making.\n",
      "*   **Finance:** AI is transforming this industry by improving efficiency and decision making.\n",
      "*   **Education:** AI is transforming this industry by improving efficiency and decision making.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "text = \"Artificial Intelligence is transforming industries like healthcare, finance, and education by improving efficiency and decision making.\"\n",
    "prompt = f\"\"\"Format the response in a list:\n",
    "Summarize the key industries Al is transforming.\n",
    "- Highlight its impact.\n",
    "Text: ```{text}```\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fca537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"name\": \"John\",\n",
      "\"profession\": \"data scientist\",\n",
      "\"skills\": [\"python\", \"machine learning\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "text = \"John is a data scientist with expertise in python and machine learning.\"\n",
    "prompt = f\"\"\"Extract key details from the text and return the output in JSON format.\n",
    "Text: ```{text}```\n",
    "Output format:\n",
    "{{\n",
    "\"name\": \",\n",
    "\"profession\"; \",\n",
    "\"skills\": []\n",
    "\n",
    "}}\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599b162",
   "metadata": {},
   "source": [
    "#### Structured paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e270a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aa5e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A balanced diet profoundly influences overall well-being, impacting both physiological function and psychological stability.\n",
      "\n",
      "### **I. Physical Health Enhancement**\n",
      "\n",
      "**A. Energy Regulation and Cellular Repair:** A diet rich in complex carbohydrates, lean proteins, and healthy fats ensures sustained energy release, preventing fatigue and supporting essential bodily functions. Furthermore, adequate protein intake is crucial for **cellular repair** and the maintenance of muscle mass, while vitamins and minerals act as cofactors in metabolic processes. **B. Chronic Disease Mitigation:** High consumption of fiber, antioxidants, and unsaturated fats significantly lowers systemic inflammation and reduces the risk of developing chronic conditions. This includes the prevention of cardiovascular disease (CVD) through cholesterol management and the stabilization of blood sugar levels, which is vital for **Type 2 diabetes prevention**.\n",
      "\n",
      "### **II. Cognitive and Emotional Well-being**\n",
      "\n",
      "**A. Neurotransmitter Synthesis:** Essential nutrients, particularly Omega-3 fatty acids (DHA/EPA) and B vitamins, are fundamental building blocks for brain structure and function. These nutrients are critical for the efficient **synthesis of neurotransmitters** like serotonin and dopamine, which directly influence mood, focus, and memory. **B. Mood Regulation and Stress Resilience:** Stable blood glucose levels, maintained by avoiding excessive refined sugars, prevent the sharp energy crashes that often trigger irritability and anxiety. Moreover, the health of the gut microbiome, supported by dietary fiber and fermented foods, plays a key role in the **gut-brain axis**, enhancing resilience against psychological stress and improving overall emotional stability.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "prompt = \"Write a structured paragraph with clear headings and subheadings about the impact of a balanced diet on physical and mental health.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d1efb",
   "metadata": {},
   "source": [
    "#### Custom Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb06e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ```In a distant Kingdom, a brave knight named Arthur set out on a quest to find the legendary sword of light. Through treacherous mountains and dark forests, he faced numerous challenges but remained determined to fulfill his destiny.```\n",
      "- Title/Summary: Knight Arthur embarks on a challenging quest for the legendary sword of light, facing obstacles but remaining determined.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "text = \"In a distant Kingdom, a brave knight named Arthur set out on a quest to find the legendary sword of light. Through treacherous mountains and dark forests, he faced numerous challenges but remained determined to fulfill his destiny.\"\n",
    "prompt= f\"\"\"You will be provided with a text delimited by triple backticks.\n",
    "If the text is short (around 20 words or fewer), generate a suitable **Title**.\n",
    "If the text is longer than 20 words, generate a concise **Summary**,\n",
    "Use the following format for the output:\n",
    "Text: <provided text>\n",
    "- Title/Summary: <generated content>\n",
    "```{text}```\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    " model = \"gemini-flash-latest\",\n",
    " contents = prompt, \n",
    " config=genai.types.GenerateContentConfig(temperature=0)\n",
    "        \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
